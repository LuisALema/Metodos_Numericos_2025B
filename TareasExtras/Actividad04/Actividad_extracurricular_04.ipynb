{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a20fee3",
   "metadata": {},
   "source": [
    "# **Escuela Politécnica Nacional**\n",
    "## **[Actividad extracurricular 04] Costos relacionados a los modelos de lenguaje**\n",
    "### **Nombre:** Luis Alexander Lema Delgado\n",
    "### **Fecha:** 02/11/2025\n",
    "### **Curso:** GR1CC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d014eba",
   "metadata": {},
   "source": [
    "# Investigación sobre Modelos de Lenguaje Comerciales\n",
    "\n",
    "En esta parte se investigaron algunas características sobre distintos modelos de lenguaje comerciales actuales, como ChatGPT, Claude, Gemini, Llama y Mistral.  \n",
    "El objetivo es comparar qué tipo de hardware utilizan, cuánto cuestan, el tiempo que tardan en entrenarse y su consumo de energía.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fd8a4",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Qué es inferencia y entrenamiento?\n",
    "\n",
    "Primero es importante entender la diferencia entre entrenamiento e inferencia.\n",
    "**Entrenamiento:**  \n",
    "Es la etapa en la que el modelo aprende a partir de una gran cantidad de texto. Durante este proceso se ajustan los parámetros internos para que el sistema pueda reconocer patrones y generar respuestas coherentes.  \n",
    "Esta fase es la más costosa y requiere miles de GPUs trabajando durante semanas o meses.\n",
    "\n",
    "**Inferencia:**  \n",
    "Ocurre cuando el modelo ya entrenado genera respuestas a partir de una entrada. Por ejemplo, cuando se le hace una pregunta a ChatGPT, el modelo realiza una inferencia.  \n",
    "Aquí ya no aprende, solo utiliza lo que sabe para dar una salida.\n",
    "\n",
    "En pocas palabras:  \n",
    "El entrenamiento es el proceso de aprendizaje, mientras que la inferencia es la aplicación de lo aprendido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21b093",
   "metadata": {},
   "source": [
    "## Aspectos a comparar\n",
    "\n",
    "Para cada modelo se consideraron los siguientes puntos:\n",
    "\n",
    "1. Tipo de GPU utilizada.  \n",
    "2. Costo total del hardware (precio de una GPU por el número de unidades).  \n",
    "3. Tiempo aproximado de entrenamiento.  \n",
    "4. Consumo energético tanto en el entrenamiento como en la inferencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cb962",
   "metadata": {},
   "source": [
    "## Tabla resumen de modelos de lenguaje\n",
    "\n",
    "*Los valores son aproximados y se basan en estimaciones públicas. No todas las empresas revelan cifras exactas.*\n",
    "\n",
    "| Modelo de Lenguaje | GPU utilizada | Costo total aprox. (USD) | Tiempo de Entrenamiento | Consumo Energético | Comentario |\n",
    "|--------------------|----------------|---------------------------|--------------------------|--------------------|-------------|\n",
    "| **ChatGPT (GPT-4)** | NVIDIA H100 (≈ $8,000 c/u) × 10,000 | ≈ $80 millones | 2 a 3 meses | Entrenamiento: ~1.2 GWh<br>Inferencia: ~0.5 kWh / 1k tokens | Modelo muy grande y costoso, pero de alto rendimiento. |\n",
    "| **Claude 3 (Anthropic)** | NVIDIA A100 | ≈ $40 millones | 2 meses | 900 MWh / 0.3 kWh | Optimizado para manejar contextos largos. |\n",
    "| **Gemini 1.5 (Google)** | TPU v5e (Google) | ≈ $50 millones | 2 a 3 meses | 1 GWh / 0.4 kWh | Usa hardware propio de Google. |\n",
    "| **Mistral 7B** | NVIDIA A100 80GB × 512 | ≈ $4 millones | 1 mes | 100 MWh / 0.1 kWh | Modelo más pequeño y de código abierto. |\n",
    "| **Llama 3 (Meta)** | NVIDIA H100 × 2,000 | ≈ $16 millones | 1.5 meses | 300 MWh / 0.2 kWh | Modelo abierto y relativamente eficiente. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11fef4f",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusión\n",
    "\n",
    "Al comparar los modelos, se puede ver que el entrenamiento es la parte más exigente en recursos.  \n",
    "Requiere gran cantidad de hardware, altos costos y mucho tiempo. Por eso, solo empresas grandes como OpenAI o Google pueden entrenar modelos de este nivel.  \n",
    "\n",
    "En cambio, la inferencia es mucho más ligera, lo que permite que los usuarios comunes podamos usar estos sistemas en línea sin requerir tanto poder de cómputo.  \n",
    "También se nota que los modelos más nuevos buscan ser más eficientes y menos costosos energéticamente, lo cual es importante para reducir el impacto ambiental y hacerlos más accesibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7bb41",
   "metadata": {},
   "source": [
    "**Resumen general:**\n",
    "- El entrenamiento es la fase donde el modelo aprende, y la inferencia es cuando usa lo aprendido.  \n",
    "- Las GPUs más usadas son las NVIDIA A100 y H100, y las TPUs en el caso de Google.  \n",
    "- Entrenar un modelo grande puede costar decenas de millones de dólares.  \n",
    "- Los nuevos modelos intentan mejorar la eficiencia energética y el acceso abierto.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
